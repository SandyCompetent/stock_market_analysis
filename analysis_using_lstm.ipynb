{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Stock Price Forecasting with LSTM\n",
    "\n",
    "This notebook provides a comprehensive approach to stock price forecasting using Long Short-Term Memory (LSTM) neural networks. Unlike simple demonstrations, this implementation includes:\n",
    "\n",
    "- **Parameterized inputs** for any stock ticker and date range\n",
    "- **Feature engineering** with technical indicators (SMA, RSI)\n",
    "- **Baseline comparison** to evaluate model effectiveness\n",
    "- **Critical performance analysis** including directional accuracy\n",
    "- **Practical insights** and limitations discussion\n",
    "\n",
    "## Parameters\n",
    "Modify these parameters to analyze different stocks and time periods:\n"
   ],
   "id": "3a5a318cb229f3a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:36.059251Z",
     "start_time": "2025-07-22T10:49:36.056945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stock Analysis Parameters\n",
    "STOCK_TICKER = 'MRK'  # Change this to any stock ticker (e.g., 'MSFT', 'GOOGL', 'TSLA')\n",
    "START_DATE = '2009-01-01'  # Start date for historical data\n",
    "END_DATE = '2020-12-31'    # End date for historical data\n",
    "\n",
    "# Model Parameters\n",
    "SEQUENCE_LENGTH = 30  # Number of days to look back for prediction\n",
    "TEST_SIZE = 0.2      # Proportion of data for testing\n",
    "EPOCHS = 50          # Number of training epochs\n",
    "BATCH_SIZE = 32      # Batch size for training\n"
   ],
   "id": "929461c802bda67c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Required Libraries\n",
   "id": "3678d0024d9bb02a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:36.201718Z",
     "start_time": "2025-07-22T10:49:36.199745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install yfinance\n",
    "# !pip install scikit-learn\n",
    "# !pip install tensorflow\n",
    "# !pip install pandas_ta\n",
    "# !pip install scipy-stubs\n",
    "# !pip install pandas-stubs"
   ],
   "id": "c2e66a40f32a01cc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:38.145421Z",
     "start_time": "2025-07-22T10:49:36.449388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Set style and suppress warnings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Analysis target: {STOCK_TICKER} from {START_DATE} to {END_DATE}\")\n"
   ],
   "id": "73b86b7d1adf76d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:49:36.869633: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-22 10:49:36.881949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753181376.896439   27390 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753181376.900772   27390 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753181376.911519   27390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753181376.911536   27390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753181376.911538   27390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753181376.911539   27390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-22 10:49:36.915661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Analysis target: MRK from 2009-01-01 to 2020-12-31\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:38.452020Z",
     "start_time": "2025-07-22T10:49:38.357648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "id": "227c3c481ac052f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:38.694661Z",
     "start_time": "2025-07-22T10:49:38.692069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ],
   "id": "b7162314f075e060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:38.939278Z",
     "start_time": "2025-07-22T10:49:38.937250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n"
   ],
   "id": "ba764e721f13bd07",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:39.142342Z",
     "start_time": "2025-07-22T10:49:39.139606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_news_dataframe(df: pd.DataFrame, file_path: str = \"stock_data.csv\") -> bool:\n",
    "    \"\"\"\n",
    "    Saves the news DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the save was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"News DataFrame saved to {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to save news DataFrame: {e}\")\n",
    "        return False\n"
   ],
   "id": "637d1d7701e8615",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Retrieval & Preprocessing\n",
    "\n",
    "We use yfinance to automatically handle historical price adjustments for stock splits and dividends, ensuring our data reflects true historical performance.\n"
   ],
   "id": "98e8c93e82d6538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:40.171292Z",
     "start_time": "2025-07-22T10:49:39.287517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = \"MRK_full_sorted_data.csv\"\n",
    "\n",
    "def fetch_static_stock_data():\n",
    "    \"\"\"\n",
    "    Fetch stock data using yfinance with automatic adjustment for splits and dividends\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data from the CSV file\n",
    "        # index_col=0: Sets the first column (our timestamps) as the DataFrame index.\n",
    "        # parse_dates=True: Converts the index column into datetime objects for time series analysis.\n",
    "        df = pd.read_csv(file_name, index_col=0, parse_dates=True)\n",
    "\n",
    "        print(f\"âœ… Data loaded successfully from '{file_name}'\")\n",
    "\n",
    "        # Display the first few rows to confirm it's loaded correctly\n",
    "        print(\"\\n--- First 5 Rows ---\")\n",
    "        print(df.head())\n",
    "\n",
    "        # You can also check the data types to ensure the index is a datetime\n",
    "        print(\"\\n--- Data Info ---\")\n",
    "        df.info()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: The file '{file_name}' was not found.\")\n",
    "        print(\"Please make sure the CSV file is in the same folder as your script.\")\n",
    "\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch stock data using yfinance with automatic adjustment for splits and dividends\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        data = stock.history(start=start_date, end=end_date, interval='90m')\n",
    "        # data = stock.h\n",
    "        # istory(period='max', interval='1d')\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data found for ticker {ticker}\")\n",
    "\n",
    "        print(f\"Successfully fetched {len(data)} days of data for {ticker}\")\n",
    "        print(f\"Date range: {data.index[0].strftime('%Y-%m-%d')} to {data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Fetch the stock data\n",
    "# stock_data = fetch_stock_data(STOCK_TICKER, START_DATE, END_DATE)\n",
    "stock_data = fetch_static_stock_data()\n",
    "print(type(stock_data))\n",
    "\n",
    "# save_news_dataframe(df=stock_data, file_path=\"stock_data_nvda.csv\")\n",
    "\n",
    "if stock_data is not None:\n",
    "    print(\"\\nStock data overview:\")\n",
    "    print(stock_data.head())\n",
    "    print(f\"\\nData shape: {stock_data.shape}\")\n",
    "    print(f\"Missing values: {stock_data.isnull().sum().sum()}\")\n"
   ],
   "id": "3371502fedc4d1c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully from 'MRK_full_sorted_data.csv'\n",
      "\n",
      "--- First 5 Rows ---\n",
      "                        High      Low    Close  Volume\n",
      "Date                                                  \n",
      "2009-01-02 08:21:00  16.2320  16.2320  16.2320     104\n",
      "2009-01-02 08:36:00  16.2374  16.2320  16.2320     209\n",
      "2009-01-02 08:38:00  16.2374  16.2320  16.2320     209\n",
      "2009-01-02 08:39:00  16.2320  16.2320  16.2320     104\n",
      "2009-01-02 08:46:00  16.3721  16.3721  16.3721    1347\n",
      "\n",
      "--- Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1238712 entries, 2009-01-02 08:21:00 to 2020-12-31 19:17:00\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   High    1238712 non-null  float64\n",
      " 1   Low     1238712 non-null  float64\n",
      " 2   Close   1238712 non-null  float64\n",
      " 3   Volume  1238712 non-null  int64  \n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 47.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Stock data overview:\n",
      "                        High      Low    Close  Volume\n",
      "Date                                                  \n",
      "2009-01-02 08:21:00  16.2320  16.2320  16.2320     104\n",
      "2009-01-02 08:36:00  16.2374  16.2320  16.2320     209\n",
      "2009-01-02 08:38:00  16.2374  16.2320  16.2320     209\n",
      "2009-01-02 08:39:00  16.2320  16.2320  16.2320     104\n",
      "2009-01-02 08:46:00  16.3721  16.3721  16.3721    1347\n",
      "\n",
      "Data shape: (1238712, 4)\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Feature Engineering with Technical Indicators\n",
    "\n",
    "Instead of using only closing prices, we'll engineer additional features that provide more context to our model:\n",
    "\n",
    "- **7-day Simple Moving Average (SMA)**: Smooths out price fluctuations\n",
    "- **Relative Strength Index (RSI)**: Measures momentum and identifies overbought/oversold conditions\n"
   ],
   "id": "845bf0cbb2f98329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:40.494811Z",
     "start_time": "2025-07-22T10:49:40.284737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_technical_indicators(data):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators for enhanced feature set\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    # 7-day Simple Moving Average\n",
    "    df['SMA_7'] = df['Close'].rolling(window=7).mean()\n",
    "\n",
    "    # Relative Strength Index (RSI)\n",
    "    def calculate_rsi(prices, window=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "\n",
    "    # Price change percentage\n",
    "    df['Price_Change_Pct'] = df['Close'].pct_change()\n",
    "\n",
    "    # Volume moving average\n",
    "    df['Volume_MA'] = df['Volume'].rolling(window=7).mean()\n",
    "\n",
    "    # High-Low spread\n",
    "    df['HL_Spread'] = (df['High'] - df['Low']) / df['Close']\n",
    "\n",
    "    # # Bollinger Bands\n",
    "    # df.ta.bbands(close='Close', length=20, append=True)\n",
    "    #\n",
    "    # # Moving Average Convergence Divergence (MACD)\n",
    "    # df.ta.macd(close='Close', append=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Calculate technical indicators\n",
    "enhanced_data = calculate_technical_indicators(stock_data)\n",
    "\n",
    "# Remove rows with NaN values (due to rolling calculations)\n",
    "enhanced_data = enhanced_data.dropna()\n",
    "\n",
    "print(f\"Enhanced dataset shape: {enhanced_data.shape}\")\n",
    "print(\"\\nNew features added:\")\n",
    "print(enhanced_data[['Close', 'SMA_7', 'RSI', 'Price_Change_Pct', 'HL_Spread']].head(10))\n"
   ],
   "id": "2d945c65c1724c49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset shape: (1238686, 9)\n",
      "\n",
      "New features added:\n",
      "                       Close      SMA_7        RSI  Price_Change_Pct  \\\n",
      "Date                                                                   \n",
      "2009-01-02 09:24:00  16.3775  16.301300  72.884555          0.005624   \n",
      "2009-01-02 09:26:00  16.3775  16.313614  72.884555          0.000000   \n",
      "2009-01-02 09:30:00  16.4927  16.342386  80.096975          0.007034   \n",
      "2009-01-02 09:31:00  16.4959  16.371614  80.242952          0.000194   \n",
      "2009-01-02 09:32:00  16.5336  16.407000  81.814346          0.002285   \n",
      "2009-01-02 09:33:00  16.5606  16.446243  76.115267          0.001633   \n",
      "2009-01-02 09:34:00  16.5821  16.488557  85.929794          0.001298   \n",
      "2009-01-02 09:35:00  16.5888  16.518743  98.248459          0.000404   \n",
      "2009-01-02 09:36:00  16.5363  16.541429  83.952328         -0.003165   \n",
      "2009-01-02 09:37:00  16.5444  16.548814  84.304690          0.000490   \n",
      "\n",
      "                     HL_Spread  \n",
      "Date                            \n",
      "2009-01-02 09:24:00   0.000000  \n",
      "2009-01-02 09:26:00   0.000000  \n",
      "2009-01-02 09:30:00   0.009471  \n",
      "2009-01-02 09:31:00   0.001958  \n",
      "2009-01-02 09:32:00   0.004560  \n",
      "2009-01-02 09:33:00   0.005199  \n",
      "2009-01-02 09:34:00   0.001948  \n",
      "2009-01-02 09:35:00   0.002918  \n",
      "2009-01-02 09:36:00   0.004269  \n",
      "2009-01-02 09:37:00   0.002932  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Data Visualization\n",
   "id": "4b02e0794f22fe0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:40.611438Z",
     "start_time": "2025-07-22T10:49:40.609118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualization# Create comprehensive visualization\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# fig.suptitle(f'{STOCK_TICKER} Stock Analysis with Technical Indicators', fontsize=16, fontweight='bold')\n",
    "#\n",
    "# # Price and SMA\n",
    "# axes[0, 0].plot(enhanced_data.index, enhanced_data['Close'], label='Close Price', alpha=0.7)\n",
    "# axes[0, 0].plot(enhanced_data.index, enhanced_data['SMA_7'], label='7-day SMA', color='red')\n",
    "# axes[0, 0].set_title('Stock Price with 7-day SMA')\n",
    "# axes[0, 0].set_ylabel('Price ($)')\n",
    "# axes[0, 0].legend()\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "#\n",
    "# # RSI\n",
    "# axes[0, 1].plot(enhanced_data.index, enhanced_data['RSI'], color='purple')\n",
    "# axes[0, 1].axhline(y=70, color='r', linestyle='--', alpha=0.7, label='Overbought (70)')\n",
    "# axes[0, 1].axhline(y=30, color='g', linestyle='--', alpha=0.7, label='Oversold (30)')\n",
    "# axes[0, 1].set_title('Relative Strength Index (RSI)')\n",
    "# axes[0, 1].set_ylabel('RSI')\n",
    "# axes[0, 1].legend()\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "#\n",
    "# # Volume\n",
    "# axes[1, 0].bar(enhanced_data.index, enhanced_data['Volume'], alpha=0.6, color='orange')\n",
    "# axes[1, 0].plot(enhanced_data.index, enhanced_data['Volume_MA'], color='red', label='Volume MA')\n",
    "# axes[1, 0].set_title('Trading Volume')\n",
    "# axes[1, 0].set_ylabel('Volume')\n",
    "# axes[1, 0].legend()\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "#\n",
    "# # Price change distribution\n",
    "# axes[1, 1].hist(enhanced_data['Price_Change_Pct'].dropna(), bins=50, alpha=0.7, color='green')\n",
    "# axes[1, 1].set_title('Daily Price Change Distribution')\n",
    "# axes[1, 1].set_xlabel('Price Change (%)')\n",
    "# axes[1, 1].set_ylabel('Frequency')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "#\n",
    "# # Display correlation matrix\n",
    "# features_for_correlation = ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread']\n",
    "# # features_for_correlation = ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread', 'BBL_20_2.0','MACD_12_26_9']\n",
    "# correlation_matrix = enhanced_data[features_for_correlation].corr()\n",
    "#\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "#             square=True, fmt='.2f')\n",
    "# plt.title('Feature Correlation Matrix')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "id": "686b21c6f29a884b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Data Scaling and Preparation\n",
    "\n",
    "**Why MinMaxScaler is crucial for LSTMs:**\n",
    "\n",
    "LSTM networks use activation functions like tanh and sigmoid that work optimally with inputs in specific ranges (typically 0-1 or -1 to 1). Without proper scaling:\n",
    "- **Gradient problems**: Large input values can cause vanishing or exploding gradients\n",
    "- **Slow convergence**: The network takes longer to learn patterns\n",
    "- **Poor performance**: Features with larger scales dominate the learning process\n",
    "\n",
    "MinMaxScaler transforms all features to the same scale (0-1), ensuring equal importance during training.\n"
   ],
   "id": "caf299c12a675bfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:42.028775Z",
     "start_time": "2025-07-22T10:49:40.759095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_lstm_data(data, sequence_length, test_size):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM training with multiple features\n",
    "    \"\"\"\n",
    "    # Select features for the model\n",
    "    feature_columns = ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread']\n",
    "    # feature_columns = ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread', 'BBL_20_2.0','MACD_12_26_9']\n",
    "    features = data[feature_columns].values\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(scaled_features)):\n",
    "        X.append(scaled_features[i-sequence_length:i])  # All features for sequence\n",
    "        y.append(scaled_features[i, 0])  # Only Close price as target\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_columns\n",
    "\n",
    "# Prepare the data\n",
    "X_train, X_test, y_train, y_test, scaler, feature_names = prepare_lstm_data(\n",
    "    enhanced_data, SEQUENCE_LENGTH, TEST_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Testing data shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"Features used: {feature_names}\")\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH} days\")\n"
   ],
   "id": "73e65f9435a9fc78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: X_train: (990924, 30, 5), y_train: (990924,)\n",
      "Testing data shape: X_test: (247732, 30, 5), y_test: (247732,)\n",
      "Features used: ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread']\n",
      "Sequence length: 30 days\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Train-Test Split Discussion\n",
    "\n",
    "**Current Approach: 80-20 Split**\n",
    "We're using a standard 80-20 split where the most recent 20% of data serves as our test set.\n",
    "\n",
    "**Why Walk-Forward Validation is Often Better for Time Series:**\n",
    "\n",
    "1. **Temporal Integrity**: Walk-forward validation respects the time-ordered nature of financial data\n",
    "2. **Realistic Testing**: It simulates real-world trading where you only have past data to predict future prices\n",
    "3. **Robust Evaluation**: Multiple test periods provide better performance estimates\n",
    "4. **Overfitting Detection**: Helps identify if the model works consistently across different market conditions\n",
    "\n",
    "**Implementation Note**: For production systems, consider implementing walk-forward validation with multiple train-test cycles, retraining the model periodically as new data becomes available.\n"
   ],
   "id": "8e87fb07cb46fa88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. LSTM Model Architecture\n",
   "id": "9f2a63df8a66f1aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:49:42.930564Z",
     "start_time": "2025-07-22T10:49:42.147125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build a stacked LSTM model with dropout layers for regularization\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First LSTM layer with return_sequences=True to stack layers\n",
    "        LSTM(units=100, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Second LSTM layer\n",
    "        LSTM(units=100, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Third LSTM layer (final layer doesn't return sequences)\n",
    "        LSTM(units=50),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Dense output layer\n",
    "        Dense(units=1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Compile with Adam optimizer and MSE loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "# Display model architecture\n",
    "print(\"LSTM Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n"
   ],
   "id": "cceaa37e49c035f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753181382.292473   27390 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 77064 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:00:09.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m100\u001B[0m)        â”‚        \u001B[38;5;34m42,400\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001B[38;5;33mDropout\u001B[0m)               â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m100\u001B[0m)        â”‚             \u001B[38;5;34m0\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)                   â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m100\u001B[0m)        â”‚        \u001B[38;5;34m80,400\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m100\u001B[0m)        â”‚             \u001B[38;5;34m0\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (\u001B[38;5;33mLSTM\u001B[0m)                   â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)             â”‚        \u001B[38;5;34m30,200\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m)             â”‚             \u001B[38;5;34m0\u001B[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001B[38;5;33mDense\u001B[0m)                   â”‚ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              â”‚            \u001B[38;5;34m51\u001B[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m153,051\u001B[0m (597.86 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,051</span> (597.86 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m153,051\u001B[0m (597.86 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,051</span> (597.86 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trainable parameters: 153,051\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Model Training with Callbacks\n",
   "id": "a9e5a194bc642379"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-22T10:49:43.254383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define callbacks for better training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {int(len(X_train) * 0.2)}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ],
   "id": "1b2d510374414f4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "Training samples: 990924, Validation samples: 198184\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753181385.505939   27529 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m 5368/24774\u001B[0m \u001B[32mâ”â”â”â”\u001B[0m\u001B[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[1m2:47\u001B[0m 9ms/step - loss: 8.3997e-04 - mae: 0.0172"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Training Performance Analysis\n",
   "id": "42155def3f183a7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "ax1.set_title('Model Loss During Training')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "ax2.plot(history.history['mae'], label='Training MAE', color='blue')\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', color='red')\n",
    "ax2.set_title('Model MAE During Training')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Absolute Error')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "overfitting_ratio = final_val_loss / final_train_loss\n",
    "\n",
    "print(f\"Final Training Loss: {final_train_loss:.6f}\")\n",
    "print(f\"Final Validation Loss: {final_val_loss:.6f}\")\n",
    "print(f\"Overfitting Ratio (Val/Train): {overfitting_ratio:.2f}\")\n",
    "\n",
    "if overfitting_ratio > 1.2:\n",
    "    print(\"âš ï¸  Warning: Model may be overfitting (validation loss > 1.2x training loss)\")\n",
    "elif overfitting_ratio < 1.1:\n",
    "    print(\"âœ… Good: Model shows minimal overfitting\")\n",
    "else:\n",
    "    print(\"âœ… Acceptable: Model shows reasonable generalization\")\n"
   ],
   "id": "ec9b69a960529d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Model Predictions and Baseline Comparison\n",
   "id": "b2da87cf70ea38a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make predictions\n",
    "train_predictions = model.predict(X_train, verbose=0)\n",
    "test_predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Create a scaler for inverse transformation (only for Close price)\n",
    "close_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "close_scaler.fit(enhanced_data[['Close']].values)\n",
    "\n",
    "# Inverse transform predictions and actual values\n",
    "train_predictions_scaled = close_scaler.inverse_transform(train_predictions)\n",
    "test_predictions_scaled = close_scaler.inverse_transform(test_predictions.reshape(-1, 1))\n",
    "y_train_scaled = close_scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = close_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create baseline predictions (naive forecast: tomorrow = today)\n",
    "# For test set, baseline is the previous day's actual price\n",
    "baseline_predictions = np.roll(y_test_scaled, 1)\n",
    "baseline_predictions[0] = y_test_scaled[0]  # Handle first prediction\n",
    "\n",
    "print(\"Predictions completed!\")\n",
    "print(f\"Train predictions shape: {train_predictions_scaled.shape}\")\n",
    "print(f\"Test predictions shape: {test_predictions_scaled.shape}\")\n",
    "print(f\"Baseline predictions shape: {baseline_predictions.shape}\")\n"
   ],
   "id": "13e1be90fd846d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Performance Metrics and Baseline Comparison\n",
   "id": "be7a103d10f06921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(actual, predicted, model_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "    # Directional Accuracy\n",
    "    actual_direction = np.diff(actual.flatten()) > 0\n",
    "    predicted_direction = np.diff(predicted.flatten()) > 0\n",
    "    directional_accuracy = np.mean(actual_direction == predicted_direction) * 100\n",
    "\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'Directional_Accuracy': directional_accuracy\n",
    "    }\n",
    "\n",
    "# Calculate metrics for both models\n",
    "lstm_metrics = calculate_metrics(y_test_scaled, test_predictions_scaled, 'LSTM')\n",
    "baseline_metrics = calculate_metrics(y_test_scaled, baseline_predictions, 'Naive Baseline')\n",
    "\n",
    "# Create comparison DataFrame\n",
    "metrics_df = pd.DataFrame([lstm_metrics, baseline_metrics])\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "print(\"ğŸ“Š PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "rmse_improvement = ((baseline_metrics['RMSE'] - lstm_metrics['RMSE']) / baseline_metrics['RMSE']) * 100\n",
    "directional_improvement = lstm_metrics['Directional_Accuracy'] - baseline_metrics['Directional_Accuracy']\n",
    "\n",
    "print(f\"\\nğŸ“ˆ MODEL EFFECTIVENESS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"RMSE Improvement over Baseline: {rmse_improvement:.2f}%\")\n",
    "print(f\"Directional Accuracy Improvement: {directional_improvement:.2f} percentage points\")\n",
    "\n",
    "if rmse_improvement > 10:\n",
    "    print(\"âœ… LSTM shows significant improvement over naive baseline\")\n",
    "elif rmse_improvement > 0:\n",
    "    print(\"âœ… LSTM shows modest improvement over naive baseline\")\n",
    "else:\n",
    "    print(\"âš ï¸  LSTM does not outperform naive baseline - consider model refinement\")\n"
   ],
   "id": "e3c143ccffb11b96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 12. Comprehensive Visualization\n",
   "id": "7d5148a63b2a1842"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create comprehensive prediction visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle(f'{STOCK_TICKER} Stock Price Prediction Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Full prediction comparison\n",
    "test_dates = enhanced_data.index[-len(y_test_scaled):]\n",
    "\n",
    "axes[0, 0].plot(test_dates, y_test_scaled, label='Actual Price', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(test_dates, test_predictions_scaled, label='LSTM Prediction', color='red', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(test_dates, baseline_predictions, label='Naive Baseline', color='green', linewidth=1, linestyle='--')\n",
    "axes[0, 0].set_title('Stock Price Predictions vs Actual')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Prediction errors\n",
    "lstm_errors = y_test_scaled.flatten() - test_predictions_scaled.flatten()\n",
    "baseline_errors = y_test_scaled.flatten() - baseline_predictions.flatten()\n",
    "\n",
    "axes[0, 1].hist(lstm_errors, bins=30, alpha=0.7, label='LSTM Errors', color='red')\n",
    "axes[0, 1].hist(baseline_errors, bins=30, alpha=0.7, label='Baseline Errors', color='green')\n",
    "axes[0, 1].set_title('Prediction Error Distribution')\n",
    "axes[0, 1].set_xlabel('Prediction Error ($)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot: Actual vs Predicted\n",
    "axes[1, 0].scatter(y_test_scaled, test_predictions_scaled, alpha=0.6, color='red', label='LSTM')\n",
    "axes[1, 0].scatter(y_test_scaled, baseline_predictions, alpha=0.6, color='green', label='Baseline')\n",
    "min_price = min(y_test_scaled.min(), test_predictions_scaled.min())\n",
    "max_price = max(y_test_scaled.max(), test_predictions_scaled.max())\n",
    "axes[1, 0].plot([min_price, max_price], [min_price, max_price], 'k--', alpha=0.8, label='Perfect Prediction')\n",
    "axes[1, 0].set_title('Actual vs Predicted Prices')\n",
    "axes[1, 0].set_xlabel('Actual Price ($)')\n",
    "axes[1, 0].set_ylabel('Predicted Price ($)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Directional accuracy over time\n",
    "window_size = 20\n",
    "lstm_directions = np.diff(test_predictions_scaled.flatten()) > 0\n",
    "actual_directions = np.diff(y_test_scaled.flatten()) > 0\n",
    "rolling_accuracy = []\n",
    "\n",
    "for i in range(window_size, len(lstm_directions)):\n",
    "    window_accuracy = np.mean(lstm_directions[i-window_size:i] == actual_directions[i-window_size:i]) * 100\n",
    "    rolling_accuracy.append(window_accuracy)\n",
    "\n",
    "rolling_dates = test_dates[window_size+1:]\n",
    "axes[1, 1].plot(rolling_dates, rolling_accuracy, color='purple', linewidth=2)\n",
    "axes[1, 1].axhline(y=50, color='red', linestyle='--', alpha=0.7, label='Random Chance')\n",
    "axes[1, 1].set_title(f'Rolling Directional Accuracy ({window_size}-day window)')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f'{STOCK_TICKER}_lstm_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nğŸ“Š Analysis chart saved as '{STOCK_TICKER}_lstm_analysis.png'\")\n"
   ],
   "id": "54705f50b6d9caf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13. Critical Analysis & Insights\n",
    "\n",
    "This section provides a thorough evaluation of our LSTM model's performance and practical implications.\n"
   ],
   "id": "5949d9cfaf4b39f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detailed performance analysis\n",
    "print(\"ğŸ” CRITICAL ANALYSIS OF LSTM PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Statistical Significance\n",
    "from scipy import stats\n",
    "\n",
    "lstm_abs_errors = np.abs(lstm_errors)\n",
    "baseline_abs_errors = np.abs(baseline_errors)\n",
    "\n",
    "# Paired t-test to check if LSTM errors are significantly different\n",
    "t_stat, p_value = stats.ttest_rel(lstm_abs_errors, baseline_abs_errors)\n",
    "\n",
    "print(f\"\\nğŸ“Š STATISTICAL SIGNIFICANCE\")\n",
    "print(f\"Paired t-test p-value: {p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"âœ… LSTM performance is statistically significantly different from baseline\")\n",
    "else:\n",
    "    print(\"âš ï¸  LSTM performance is not statistically significantly different from baseline\")\n",
    "\n",
    "# 2. Directional Accuracy Analysis\n",
    "print(f\"\\nğŸ¯ DIRECTIONAL ACCURACY BREAKDOWN\")\n",
    "print(f\"LSTM Directional Accuracy: {lstm_metrics['Directional_Accuracy']:.2f}%\")\n",
    "print(f\"Baseline Directional Accuracy: {baseline_metrics['Directional_Accuracy']:.2f}%\")\n",
    "\n",
    "if lstm_metrics['Directional_Accuracy'] > 55:\n",
    "    print(\"âœ… Strong directional prediction capability\")\n",
    "elif lstm_metrics['Directional_Accuracy'] > 50:\n",
    "    print(\"âœ… Modest directional prediction capability\")\n",
    "else:\n",
    "    print(\"âš ï¸  Poor directional prediction - worse than random\")\n",
    "\n",
    "# 3. Volatility Analysis\n",
    "actual_volatility = np.std(y_test_scaled)\n",
    "predicted_volatility = np.std(test_predictions_scaled)\n",
    "volatility_ratio = predicted_volatility / actual_volatility\n",
    "\n",
    "print(f\"\\nğŸ“ˆ VOLATILITY ANALYSIS\")\n",
    "print(f\"Actual Price Volatility: ${actual_volatility:.2f}\")\n",
    "print(f\"Predicted Price Volatility: ${predicted_volatility:.2f}\")\n",
    "print(f\"Volatility Ratio (Pred/Actual): {volatility_ratio:.2f}\")\n",
    "\n",
    "if 0.8 <= volatility_ratio <= 1.2:\n",
    "    print(\"âœ… Model captures volatility well\")\n",
    "elif volatility_ratio < 0.8:\n",
    "    print(\"âš ï¸  Model underestimates volatility (too conservative)\")\n",
    "else:\n",
    "    print(\"âš ï¸  Model overestimates volatility (too aggressive)\")\n",
    "\n",
    "# 4. Trend Following Analysis\n",
    "y_test_float32 = y_test_scaled.flatten().astype('float32')\n",
    "predictions_float32 = test_predictions_scaled.flatten().astype('float32')\n",
    "\n",
    "actual_trend = np.polyfit(range(len(y_test_float32)), y_test_float32, 1)[0]\n",
    "predicted_trend = np.polyfit(range(len(predictions_float32)), predictions_float32, 1)[0]\n",
    "\n",
    "print(f\"\\nğŸ“Š TREND ANALYSIS\")\n",
    "print(f\"Actual Trend ($/day): {actual_trend:.4f}\")\n",
    "print(f\"Predicted Trend ($/day): {predicted_trend:.4f}\")\n",
    "print(f\"Trend Capture Ratio: {predicted_trend/actual_trend:.2f}\" if actual_trend != 0 else \"Trend Capture: N/A (flat trend)\")\n",
    "\n",
    "# 5. Error Analysis by Market Conditions\n",
    "price_changes = np.diff(y_test_scaled.flatten())\n",
    "up_days = price_changes > 0\n",
    "down_days = price_changes < 0\n",
    "\n",
    "if len(price_changes) > 1:\n",
    "    up_day_errors = lstm_errors[1:][up_days]\n",
    "    down_day_errors = lstm_errors[1:][down_days]\n",
    "\n",
    "    print(f\"\\nğŸ“Š PERFORMANCE BY MARKET CONDITION\")\n",
    "    if len(up_day_errors) > 0:\n",
    "        print(f\"Average error on up days: ${np.mean(np.abs(up_day_errors)):.2f}\")\n",
    "    if len(down_day_errors) > 0:\n",
    "        print(f\"Average error on down days: ${np.mean(np.abs(down_day_errors)):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ],
   "id": "e7796b3979d5f8b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 14. Model Limitations & Real-World Considerations\n",
    "\n",
    "### ğŸš¨ **Critical Limitations**\n",
    "\n",
    "1. **Historical Bias**: The model is trained exclusively on past data and assumes historical patterns will continue\n",
    "\n",
    "2. **Black Swan Events**: Cannot predict unprecedented market events (crashes, pandemics, geopolitical crises)\n",
    "\n",
    "3. **Market Regime Changes**: May fail when market dynamics fundamentally shift\n",
    "\n",
    "4. **Feature Limitations**: Only uses price and volume data - ignores fundamental analysis, news sentiment, macroeconomic factors\n",
    "\n",
    "5. **Overfitting Risk**: Complex models may memorize noise rather than learn genuine patterns\n",
    "\n",
    "6. **Transaction Costs**: Real trading involves spreads, commissions, and slippage not accounted for in predictions\n",
    "\n",
    "7. **Market Impact**: Large trades based on model predictions could move prices, invalidating the predictions\n",
    "\n",
    "### âš–ï¸ **Regulatory and Ethical Considerations**\n",
    "\n",
    "- **Not Financial Advice**: This model is for educational purposes only\n",
    "- **Risk Management**: Never risk more than you can afford to lose\n",
    "- **Diversification**: Don't rely on a single model or asset\n",
    "- **Continuous Monitoring**: Model performance can degrade over time\n"
   ],
   "id": "fb8034cd244ecc01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 15. Next Steps for Model Improvement\n",
    "\n",
    "### ğŸ”§ **Immediate Improvements**\n",
    "\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Grid search for optimal LSTM units, dropout rates, learning rates\n",
    "   - Experiment with different sequence lengths\n",
    "   - Try different optimizers (RMSprop, AdaGrad)\n",
    "\n",
    "2. **Alternative Architectures**:\n",
    "   - **GRU (Gated Recurrent Unit)**: Often performs similarly to LSTM with fewer parameters\n",
    "   - **Bidirectional LSTM**: Processes sequences in both directions\n",
    "   - **Attention Mechanisms**: Focus on most relevant time steps\n",
    "   - **Transformer Models**: State-of-the-art for sequence modeling\n",
    "\n",
    "3. **Enhanced Features**:\n",
    "   - **More Technical Indicators**: MACD, Bollinger Bands, Stochastic Oscillator\n",
    "   - **Market Sentiment**: VIX (fear index), put/call ratios\n",
    "   - **Fundamental Data**: P/E ratios, earnings, revenue growth\n",
    "   - **Macroeconomic Indicators**: Interest rates, inflation, GDP growth\n",
    "\n",
    "### ğŸš€ **Advanced Enhancements**\n",
    "\n",
    "4. **Multi-Asset Models**:\n",
    "   - Include correlated assets (sector ETFs, commodities)\n",
    "   - Cross-asset attention mechanisms\n",
    "\n",
    "5. **News and Sentiment Integration**:\n",
    "   - Natural Language Processing on financial news\n",
    "   - Social media sentiment analysis\n",
    "   - Earnings call transcripts analysis\n",
    "\n",
    "6. **Ensemble Methods**:\n",
    "   - Combine multiple models (LSTM + Random Forest + Linear Regression)\n",
    "   - Weighted voting based on recent performance\n",
    "\n",
    "7. **Online Learning**:\n",
    "   - Continuously update model with new data\n",
    "   - Adaptive learning rates based on market conditions\n",
    "\n",
    "### ğŸ“Š **Validation Improvements**\n",
    "\n",
    "8. **Walk-Forward Validation**:\n",
    "   - Multiple train-test cycles\n",
    "   - Out-of-sample testing across different market conditions\n",
    "\n",
    "9. **Risk-Adjusted Metrics**:\n",
    "   - Sharpe ratio, Sortino ratio\n",
    "   - Maximum drawdown analysis\n",
    "   - Value at Risk (VaR) calculations\n"
   ],
   "id": "d4cfd66169426025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final summary and recommendations\n",
    "print(\"ğŸ¯ FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if rmse_improvement > 15 and lstm_metrics['Directional_Accuracy'] > 55:\n",
    "    print(\"âœ… STRONG MODEL: Consider for further development\")\n",
    "    print(\"   â†’ Focus on hyperparameter tuning and feature engineering\")\n",
    "    print(\"   â†’ Implement walk-forward validation\")\n",
    "    print(\"   â†’ Add risk management components\")\n",
    "\n",
    "elif rmse_improvement > 5 and lstm_metrics['Directional_Accuracy'] > 50:\n",
    "    print(\"âœ… PROMISING MODEL: Needs refinement\")\n",
    "    print(\"   â†’ Try alternative architectures (GRU, Attention)\")\n",
    "    print(\"   â†’ Add more features (sentiment, fundamentals)\")\n",
    "    print(\"   â†’ Implement ensemble methods\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  WEAK MODEL: Significant improvements needed\")\n",
    "    print(\"   â†’ Reconsider feature selection\")\n",
    "    print(\"   â†’ Try completely different approaches\")\n",
    "    print(\"   â†’ Consider if this asset is predictable with current methods\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ MODEL PERFORMANCE SUMMARY FOR {STOCK_TICKER}\")\n",
    "print(f\"   RMSE: ${lstm_metrics['RMSE']:.2f}\")\n",
    "print(f\"   Directional Accuracy: {lstm_metrics['Directional_Accuracy']:.1f}%\")\n",
    "print(f\"   Improvement over Baseline: {rmse_improvement:.1f}%\")\n",
    "\n",
    "print(\"\\nâš ï¸  REMEMBER: This is for educational purposes only.\")\n",
    "print(\"   Always conduct thorough backtesting and risk assessment\")\n",
    "print(\"   before considering any real-world application.\")\n",
    "\n",
    "print(\"\\nğŸ‰ Analysis Complete! Thank you for using this comprehensive LSTM stock forecasting notebook.\")\n"
   ],
   "id": "81293fb43c3b5165",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c86a04ebc8b6cc7c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
