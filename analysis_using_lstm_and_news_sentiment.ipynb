{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Stock Forecasting with LSTM and News Sentiment Analysis\n",
    "\n",
    "This comprehensive notebook merges LSTM-based stock price forecasting with news sentiment analysis to create an advanced predictive model. The workflow includes:\n",
    "\n",
    "1. **Automated Stock Selection** from news data\n",
    "2. **Sentiment Analysis** using FinBERT on news headlines\n",
    "3. **Feature Engineering** with sentiment scores and technical indicators\n",
    "4. **Baseline LSTM Model** using only technical indicators\n",
    "5. **Enhanced LSTM Model** incorporating news sentiment\n",
    "6. **Performance Comparison** between both models\n"
   ],
   "id": "235b4cf80a699ae9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Required Libraries and Configuration\n",
   "id": "996f190e822aa16d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install yfinance\n",
    "# !pip install scikit-learn\n",
    "# !pip install tensorflow\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install ipywidgets"
   ],
   "id": "308592fefe4756c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# NLP Libraries for sentiment analysis\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ],
   "id": "42726c88d82b9297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "id": "4baf34f403f2ec51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ],
   "id": "b6ff6ddcb92947ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ],
   "id": "a0e5d888e99c6bc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Automated Stock Selection from News Data\n",
   "id": "3c3aea024e7070c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def load_and_analyze_news_data(file_path='news_data.csv'):\n",
    "#     \"\"\"\n",
    "#     Load news data and identify the most frequent stock ticker\n",
    "#     \"\"\"\n",
    "#     print(\"Loading news data...\")\n",
    "#     df = pd.read_csv(file_path)\n",
    "#\n",
    "#     print(f\"Dataset shape: {df.shape}\")\n",
    "#     print(\"\\nTop 10 most frequent stocks in news:\")\n",
    "#     stock_counts = df['stock'].value_counts().head(10)\n",
    "#     print(stock_counts)\n",
    "#\n",
    "#     # Get the most frequent stock\n",
    "#     target_stock = stock_counts.index[3]\n",
    "#     print(f\"\\n🎯 Selected target stock: {target_stock} ({stock_counts.iloc[3]} articles)\")\n",
    "#\n",
    "#     return df, target_stock\n",
    "#\n",
    "# import pandas as pd\n",
    "\n",
    "def load_and_analyze_news_data(file_path='news_data.csv'):\n",
    "    \"\"\"\n",
    "    Load news data, identify the top 10 stocks, and show their date ranges.\n",
    "    \"\"\"\n",
    "    print(\"Loading news data...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # --- CHANGE 1: Convert 'date' column to datetime objects ---\n",
    "    # This is crucial for correctly finding the earliest (min) and latest (max) dates.\n",
    "    try:\n",
    "        df['Date'] = pd.to_datetime(df['date'], format='mixed', utc=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting dates: {e}\")\n",
    "        # Try a more robust approach with error handling for problematic values\n",
    "        df['Date'] = pd.to_datetime(df['date'], format='mixed', utc=True, errors='coerce')\n",
    "        # Drop rows with NaT (Not a Time) values if any\n",
    "        df = df.dropna(subset=['Date'])\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "    # --- CHANGE 2: Use groupby to get count, start, and end dates together ---\n",
    "    # This is more efficient than filtering for each stock individually.\n",
    "    stock_analysis = df.groupby('stock')['date'].agg(\n",
    "        count='size',          # Get the count of articles for each stock\n",
    "        start_date='min',      # Find the earliest date\n",
    "        end_date='max'         # Find the latest date\n",
    "    ).sort_values('count', ascending=False) # Sort by count to find the most frequent\n",
    "\n",
    "    # Get the top 10\n",
    "    top_10_stocks = stock_analysis.head(10)\n",
    "\n",
    "    print(\"\\nTop 10 most frequent stocks with date ranges:\")\n",
    "    print(top_10_stocks)\n",
    "\n",
    "    # Get the 4th most frequent stock, same as your original code\n",
    "    target_stock = top_10_stocks.index[3]\n",
    "    print(f\"\\n🎯 Selected target stock: {target_stock} ({top_10_stocks.loc[target_stock, 'count']} articles)\")\n",
    "\n",
    "    return df, target_stock\n",
    "\n",
    "# Load news data and select target stock\n",
    "# Make sure you have a 'news_data.csv' file in the same directory\n",
    "# news_df, TARGET_STOCK = load_and_analyze_news_data()\n",
    "\n",
    "# Load news data and select target stock\n",
    "news_df, TARGET_STOCK = load_and_analyze_news_data()\n"
   ],
   "id": "805806dc17150b68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Sentiment Analysis and Feature Engineering\n",
   "id": "252278dcb49fdeb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize FinBERT model for sentiment analysis\n",
    "print(\"Loading FinBERT model...\")\n",
    "finbert_model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(finbert_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finbert_model_name)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for sentiment analysis\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', '_')\n",
    "    text = text.replace(\"'s\", '')\n",
    "    text = re.sub(r'[^a-zA-Z\\s_]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def analyze_sentiment(description):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using FinBERT\n",
    "    Returns: 'Positive', 'Negative', 'Neutral', or 'Unknown'\n",
    "    \"\"\"\n",
    "    if not description:\n",
    "        return 'Unknown'\n",
    "    \n",
    "    try:\n",
    "        cleaned_description = clean_text(description)\n",
    "        if not cleaned_description:\n",
    "            return 'Unknown'\n",
    "        \n",
    "        inputs = tokenizer(cleaned_description, return_tensors=\"pt\", \n",
    "                          padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "        return sentiment_map[predicted_class]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return 'Unknown'\n",
    "\n",
    "print(\"FinBERT model loaded successfully!\")\n"
   ],
   "id": "3e26bd258148138e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_news_sentiment(news_df, target_stock):\n",
    "    \"\"\"\n",
    "    Filter news for target company and process sentiment\n",
    "    \"\"\"\n",
    "    print(f\"\\n📰 Processing news sentiment for {target_stock}...\")\n",
    "\n",
    "    # Filter news for target company\n",
    "    company_news = news_df[news_df['stock'] == target_stock].copy()\n",
    "    print(f\"Found {len(company_news)} news articles for {target_stock}\")\n",
    "\n",
    "    if company_news.empty:\n",
    "        print(f\"No news found for {target_stock}\")\n",
    "        return None\n",
    "\n",
    "    # Clean and parse dates with mixed format handling\n",
    "    print(\"Processing dates...\")\n",
    "    company_news['date'] = pd.to_datetime(company_news['date'], utc=True, errors='coerce')\n",
    "    company_news = company_news.dropna(subset=['date'])\n",
    "    company_news['Date'] = company_news['date'].dt.date\n",
    "\n",
    "    print(f\"Date range: {company_news['date'].min()} to {company_news['date'].max()}\")\n",
    "\n",
    "    # Apply sentiment analysis to titles\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    company_news['Sentiment'] = company_news['title'].apply(analyze_sentiment)\n",
    "\n",
    "    # Convert sentiment to numerical scores\n",
    "    sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1, 'Unknown': 0}\n",
    "    company_news['Sentiment_Score'] = company_news['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "    # Display sentiment distribution\n",
    "    sentiment_counts = company_news['Sentiment'].value_counts()\n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(company_news)) * 100\n",
    "        print(f\"  {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    return company_news\n",
    "\n",
    "# Process sentiment for target company\n",
    "company_sentiment_df = process_news_sentiment(news_df, TARGET_STOCK)\n"
   ],
   "id": "de8079d56501143a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def aggregate_daily_sentiment(sentiment_df):\n",
    "    \"\"\"\n",
    "    Aggregate sentiment scores on a daily basis\n",
    "    \"\"\"\n",
    "    if sentiment_df is None or sentiment_df.empty:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n📊 Aggregating daily sentiment scores...\")\n",
    "    \n",
    "    # Group by date and calculate daily sentiment metrics\n",
    "    daily_sentiment = sentiment_df.groupby('Date').agg({\n",
    "        'Sentiment_Score': ['mean', 'sum', 'count'],\n",
    "        'Sentiment': lambda x: (x == 'Positive').sum(),  # Count of positive news\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    daily_sentiment.columns = ['Avg_Sentiment', 'Total_Sentiment', 'News_Count', 'Positive_Count']\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    daily_sentiment['Negative_Count'] = sentiment_df.groupby('Date')['Sentiment'].apply(\n",
    "        lambda x: (x == 'Negative').sum()\n",
    "    )\n",
    "    daily_sentiment['Neutral_Count'] = sentiment_df.groupby('Date')['Sentiment'].apply(\n",
    "        lambda x: (x == 'Neutral').sum()\n",
    "    )\n",
    "    \n",
    "    # Calculate sentiment ratio (positive - negative) / total\n",
    "    daily_sentiment['Sentiment_Ratio'] = (\n",
    "        (daily_sentiment['Positive_Count'] - daily_sentiment['Negative_Count']) / \n",
    "        daily_sentiment['News_Count']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(f\"Daily sentiment data shape: {daily_sentiment.shape}\")\n",
    "    print(f\"Date range: {daily_sentiment.index.min()} to {daily_sentiment.index.max()}\")\n",
    "    \n",
    "    return daily_sentiment\n",
    "\n",
    "# Aggregate daily sentiment\n",
    "daily_sentiment_df = aggregate_daily_sentiment(company_sentiment_df)\n",
    "\n",
    "if daily_sentiment_df is not None:\n",
    "    print(\"\\nSample daily sentiment data:\")\n",
    "    print(daily_sentiment_df.head(10))\n"
   ],
   "id": "a0bc505e84355bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Dynamic Date Range Calculation\n",
   "id": "f753c9d0de066f57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_dynamic_date_range(daily_sentiment_df):\n",
    "    \"\"\"\n",
    "    Calculate dynamic date range based on news data\n",
    "    \"\"\"\n",
    "    if daily_sentiment_df is None or daily_sentiment_df.empty:\n",
    "        print(\"No sentiment data available for date range calculation\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get earliest and latest news dates\n",
    "    earliest_news_date = daily_sentiment_df.index.min()\n",
    "    latest_news_date = daily_sentiment_df.index.max()\n",
    "    \n",
    "    # Set START_DATE to one year before earliest news date\n",
    "    start_date = earliest_news_date - pd.DateOffset(years=1)\n",
    "    \n",
    "    print(f\"\\n📅 Dynamic Date Range Calculation:\")\n",
    "    print(f\"Earliest news date: {earliest_news_date}\")\n",
    "    print(f\"Latest news date: {latest_news_date}\")\n",
    "    print(f\"LSTM START_DATE (1 year before): {start_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"LSTM END_DATE: {latest_news_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    return start_date.strftime('%Y-%m-%d'), latest_news_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Calculate dynamic date range\n",
    "START_DATE, END_DATE = calculate_dynamic_date_range(daily_sentiment_df)\n",
    "\n",
    "# Model Parameters\n",
    "SEQUENCE_LENGTH = 30  # Number of days to look back for prediction\n",
    "TEST_SIZE = 0.2      # Proportion of data for testing\n",
    "EPOCHS = 50          # Number of training epochs\n",
    "BATCH_SIZE = 32      # Batch size for training\n",
    "\n",
    "print(f\"\\n🎯 Analysis Configuration:\")\n",
    "print(f\"Target Stock: {TARGET_STOCK}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Sequence Length: {SEQUENCE_LENGTH} days\")\n",
    "print(f\"Test Size: {TEST_SIZE}\")\n"
   ],
   "id": "cb55cd02e9d5dbde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Report 1: Baseline LSTM Model (Technical Indicators Only)\n",
    "\n",
    "This section implements the baseline LSTM model using only historical price data and technical indicators, without news sentiment.\n"
   ],
   "id": "59e7880cbf115122"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch stock data using yfinance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        data = stock.history(start=start_date, end=end_date, interval='1d')\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data found for ticker {ticker}\")\n",
    "        \n",
    "        print(f\"Successfully fetched {len(data)} days of data for {ticker}\")\n",
    "        print(f\"Date range: {data.index[0].strftime('%Y-%m-%d')} to {data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_technical_indicators(data):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators for enhanced feature set\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # 7-day Simple Moving Average\n",
    "    df['SMA_7'] = df['Close'].rolling(window=7).mean()\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    def calculate_rsi(prices, window=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # Price change percentage\n",
    "    df['Price_Change_Pct'] = df['Close'].pct_change()\n",
    "    \n",
    "    # Volume moving average\n",
    "    df['Volume_MA'] = df['Volume'].rolling(window=7).mean()\n",
    "    \n",
    "    # High-Low spread\n",
    "    df['HL_Spread'] = (df['High'] - df['Low']) / df['Close']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fetch stock data\n",
    "print(f\"\\n📈 Fetching stock data for {TARGET_STOCK}...\")\n",
    "stock_data = fetch_stock_data(TARGET_STOCK, START_DATE, END_DATE)\n",
    "\n",
    "if stock_data is not None:\n",
    "    # Calculate technical indicators\n",
    "    enhanced_data = calculate_technical_indicators(stock_data)\n",
    "    enhanced_data = enhanced_data.dropna()\n",
    "    \n",
    "    print(f\"Enhanced dataset shape: {enhanced_data.shape}\")\n",
    "    print(\"\\nTechnical indicators calculated:\")\n",
    "    print(enhanced_data[['Close', 'SMA_7', 'RSI', 'Price_Change_Pct', 'HL_Spread']].head())\n"
   ],
   "id": "e791bbdf15c43e2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_lstm_data(data, sequence_length, test_size):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM training with multiple features\n",
    "    \"\"\"\n",
    "    # Select features for the model\n",
    "    feature_columns = ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread']\n",
    "    features = data[feature_columns].values\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(scaled_features)):\n",
    "        X.append(scaled_features[i-sequence_length:i])  # All features for sequence\n",
    "        y.append(scaled_features[i, 0])  # Only Close price as target\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_columns\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build a stacked LSTM model with dropout layers for regularization\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First LSTM layer with return_sequences=True to stack layers\n",
    "        LSTM(units=100, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(units=100, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Third LSTM layer (final layer doesn't return sequences)\n",
    "        LSTM(units=50),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Dense output layer\n",
    "        Dense(units=1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compile with Adam optimizer and MSE loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare baseline data (technical indicators only)\n",
    "print(\"\\n🔧 Preparing baseline LSTM data...\")\n",
    "X_train_baseline, X_test_baseline, y_train_baseline, y_test_baseline, scaler_baseline, feature_names_baseline = prepare_lstm_data(\n",
    "    enhanced_data, SEQUENCE_LENGTH, TEST_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Baseline Training data shape: X_train: {X_train_baseline.shape}, y_train: {y_train_baseline.shape}\")\n",
    "print(f\"Baseline Testing data shape: X_test: {X_test_baseline.shape}, y_test: {y_test_baseline.shape}\")\n",
    "print(f\"Features used: {feature_names_baseline}\")\n",
    "\n",
    "# Build baseline model\n",
    "print(\"\\n🏗️ Building baseline LSTM model...\")\n",
    "baseline_model = build_lstm_model((X_train_baseline.shape[1], X_train_baseline.shape[2]))\n",
    "print(\"Baseline LSTM Model Architecture:\")\n",
    "baseline_model.summary()\n"
   ],
   "id": "bdb250b59c6e49d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train baseline model\n",
    "print(\"\\n🚀 Training baseline LSTM model...\")\n",
    "\n",
    "# Define callbacks for better training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_history = baseline_model.fit(\n",
    "    X_train_baseline, y_train_baseline,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Baseline model training completed!\")\n"
   ],
   "id": "df82e6ee5dce80f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(actual, predicted, model_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    \n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    # Directional Accuracy\n",
    "    actual_direction = np.diff(actual.flatten()) > 0\n",
    "    predicted_direction = np.diff(predicted.flatten()) > 0\n",
    "    directional_accuracy = np.mean(actual_direction == predicted_direction) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'Directional_Accuracy': directional_accuracy\n",
    "    }\n",
    "\n",
    "# Make baseline predictions\n",
    "print(\"\\n📊 Evaluating baseline model...\")\n",
    "baseline_train_predictions = baseline_model.predict(X_train_baseline, verbose=0)\n",
    "baseline_test_predictions = baseline_model.predict(X_test_baseline, verbose=0)\n",
    "\n",
    "# Create a scaler for inverse transformation (only for Close price)\n",
    "close_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "close_scaler.fit(enhanced_data[['Close']].values)\n",
    "\n",
    "# Inverse transform predictions and actual values\n",
    "baseline_train_predictions_scaled = close_scaler.inverse_transform(baseline_train_predictions)\n",
    "baseline_test_predictions_scaled = close_scaler.inverse_transform(baseline_test_predictions.reshape(-1, 1))\n",
    "y_train_baseline_scaled = close_scaler.inverse_transform(y_train_baseline.reshape(-1, 1))\n",
    "y_test_baseline_scaled = close_scaler.inverse_transform(y_test_baseline.reshape(-1, 1))\n",
    "\n",
    "# Create naive baseline predictions (tomorrow = today)\n",
    "naive_predictions = np.roll(y_test_baseline_scaled, 1)\n",
    "naive_predictions[0] = y_test_baseline_scaled[0]\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_lstm_metrics = calculate_metrics(y_test_baseline_scaled, baseline_test_predictions_scaled, 'Baseline LSTM')\n",
    "naive_baseline_metrics = calculate_metrics(y_test_baseline_scaled, naive_predictions, 'Naive Baseline')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📈 REPORT 1: STOCK FORECAST USING TECHNICAL INDICATORS ONLY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "baseline_metrics_df = pd.DataFrame([baseline_lstm_metrics, naive_baseline_metrics])\n",
    "baseline_metrics_df = baseline_metrics_df.round(4)\n",
    "\n",
    "print(\"\\n📊 PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "print(baseline_metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvement over naive baseline\n",
    "rmse_improvement_baseline = ((naive_baseline_metrics['RMSE'] - baseline_lstm_metrics['RMSE']) / naive_baseline_metrics['RMSE']) * 100\n",
    "directional_improvement_baseline = baseline_lstm_metrics['Directional_Accuracy'] - naive_baseline_metrics['Directional_Accuracy']\n",
    "\n",
    "print(f\"\\n📈 MODEL EFFECTIVENESS\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"RMSE Improvement over Naive Baseline: {rmse_improvement_baseline:.2f}%\")\n",
    "print(f\"Directional Accuracy Improvement: {directional_improvement_baseline:.2f} percentage points\")\n",
    "\n",
    "if rmse_improvement_baseline > 10:\n",
    "    print(\"✅ Baseline LSTM shows significant improvement over naive baseline\")\n",
    "elif rmse_improvement_baseline > 0:\n",
    "    print(\"✅ Baseline LSTM shows modest improvement over naive baseline\")\n",
    "else:\n",
    "    print(\"⚠️  Baseline LSTM does not outperform naive baseline\")\n"
   ],
   "id": "e9563afb728dd318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization for baseline model\n",
    "print(\"\\n📊 Creating baseline model visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle(f'{TARGET_STOCK} Baseline LSTM Analysis (Technical Indicators Only)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Training history\n",
    "axes[0, 0].plot(baseline_history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[0, 0].plot(baseline_history.history['val_loss'], label='Validation Loss', color='red')\n",
    "axes[0, 0].set_title('Model Loss During Training')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions vs Actual\n",
    "test_dates = enhanced_data.index[-len(y_test_baseline_scaled):]\n",
    "axes[0, 1].plot(test_dates, y_test_baseline_scaled, label='Actual Price', color='blue', linewidth=2)\n",
    "axes[0, 1].plot(test_dates, baseline_test_predictions_scaled, label='LSTM Prediction', color='red', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].plot(test_dates, naive_predictions, label='Naive Baseline', color='green', linewidth=1, linestyle='--')\n",
    "axes[0, 1].set_title('Stock Price Predictions vs Actual')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Prediction errors\n",
    "lstm_errors = y_test_baseline_scaled.flatten() - baseline_test_predictions_scaled.flatten()\n",
    "naive_errors = y_test_baseline_scaled.flatten() - naive_predictions.flatten()\n",
    "\n",
    "axes[1, 0].hist(lstm_errors, bins=30, alpha=0.7, label='LSTM Errors', color='red')\n",
    "axes[1, 0].hist(naive_errors, bins=30, alpha=0.7, label='Naive Errors', color='green')\n",
    "axes[1, 0].set_title('Prediction Error Distribution')\n",
    "axes[1, 0].set_xlabel('Prediction Error ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter plot: Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test_baseline_scaled, baseline_test_predictions_scaled, alpha=0.6, color='red', label='LSTM')\n",
    "axes[1, 1].scatter(y_test_baseline_scaled, naive_predictions, alpha=0.6, color='green', label='Naive')\n",
    "min_price = min(y_test_baseline_scaled.min(), baseline_test_predictions_scaled.min())\n",
    "max_price = max(y_test_baseline_scaled.max(), baseline_test_predictions_scaled.max())\n",
    "axes[1, 1].plot([min_price, max_price], [min_price, max_price], 'k--', alpha=0.8, label='Perfect Prediction')\n",
    "axes[1, 1].set_title('Actual vs Predicted Prices')\n",
    "axes[1, 1].set_xlabel('Actual Price ($)')\n",
    "axes[1, 1].set_ylabel('Predicted Price ($)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Report 1 completed for {TARGET_STOCK}\")\n",
    "print(f\"Baseline LSTM RMSE: ${baseline_lstm_metrics['RMSE']:.2f}\")\n",
    "print(f\"Baseline LSTM Directional Accuracy: {baseline_lstm_metrics['Directional_Accuracy']:.1f}%\")\n"
   ],
   "id": "5447e2e09c3376ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Report 2: Sentiment-Enhanced LSTM Model\n",
    "\n",
    "This section implements the enhanced LSTM model that incorporates news sentiment as an additional predictive feature alongside technical indicators.\n"
   ],
   "id": "aca0f55b5ad17f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_enhanced_dataset(stock_data, daily_sentiment_df):\n",
    "    \"\"\"\n",
    "    Create enhanced dataset by merging stock data with daily sentiment scores\n",
    "    \"\"\"\n",
    "    print(\"\\n🔗 Creating sentiment-enhanced dataset...\")\n",
    "    \n",
    "    if daily_sentiment_df is None or daily_sentiment_df.empty:\n",
    "        print(\"No sentiment data available for enhancement\")\n",
    "        return None\n",
    "    \n",
    "    # Create a copy of stock data\n",
    "    enhanced_stock_data = stock_data.copy()\n",
    "    \n",
    "    # Convert stock data index to date for merging\n",
    "    enhanced_stock_data['Date'] = enhanced_stock_data.index.date\n",
    "    enhanced_stock_data = enhanced_stock_data.set_index('Date')\n",
    "    \n",
    "    # Merge with sentiment data\n",
    "    merged_data = enhanced_stock_data.join(daily_sentiment_df, how='left')\n",
    "    \n",
    "    # Handle missing sentiment data\n",
    "    print(\"Handling missing sentiment data...\")\n",
    "    \n",
    "    # Forward fill sentiment scores for days with no news\n",
    "    sentiment_columns = ['Avg_Sentiment', 'Total_Sentiment', 'News_Count', \n",
    "                        'Positive_Count', 'Negative_Count', 'Neutral_Count', 'Sentiment_Ratio']\n",
    "    \n",
    "    for col in sentiment_columns:\n",
    "        if col in merged_data.columns:\n",
    "            # Forward fill first, then backward fill for any remaining NaN at the beginning\n",
    "            merged_data[col] = merged_data[col].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Reset index to datetime for consistency\n",
    "    merged_data.index = pd.to_datetime(merged_data.index)\n",
    "    \n",
    "    print(f\"Enhanced dataset shape: {merged_data.shape}\")\n",
    "    print(f\"Sentiment columns added: {[col for col in sentiment_columns if col in merged_data.columns]}\")\n",
    "    \n",
    "    # Display sample of enhanced data\n",
    "    print(\"\\nSample enhanced data:\")\n",
    "    display_cols = ['Close', 'SMA_7', 'RSI', 'Avg_Sentiment', 'News_Count', 'Sentiment_Ratio']\n",
    "    available_cols = [col for col in display_cols if col in merged_data.columns]\n",
    "    print(merged_data[available_cols].head(10))\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# Create enhanced dataset\n",
    "enhanced_stock_data = create_enhanced_dataset(enhanced_data, daily_sentiment_df)\n"
   ],
   "id": "e737da24c31db4e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_enhanced_lstm_data(data, sequence_length, test_size):\n",
    "    \"\"\"\n",
    "    Prepare enhanced data for LSTM training with sentiment features\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    # Select features including sentiment\n",
    "    base_features = ['Close', 'SMA_7', 'RSI', 'Volume', 'HL_Spread']\n",
    "    sentiment_features = ['Avg_Sentiment', 'Sentiment_Ratio', 'News_Count']\n",
    "    \n",
    "    # Check which sentiment features are available\n",
    "    available_sentiment_features = [col for col in sentiment_features if col in data.columns]\n",
    "    feature_columns = base_features + available_sentiment_features\n",
    "    \n",
    "    print(f\"Enhanced features: {feature_columns}\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = data[feature_columns].values\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(scaled_features)):\n",
    "        X.append(scaled_features[i-sequence_length:i])  # All features for sequence\n",
    "        y.append(scaled_features[i, 0])  # Only Close price as target\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_columns\n",
    "\n",
    "# Prepare enhanced data\n",
    "if enhanced_stock_data is not None:\n",
    "    print(\"\\n🔧 Preparing enhanced LSTM data...\")\n",
    "    X_train_enhanced, X_test_enhanced, y_train_enhanced, y_test_enhanced, scaler_enhanced, feature_names_enhanced = prepare_enhanced_lstm_data(\n",
    "        enhanced_stock_data, SEQUENCE_LENGTH, TEST_SIZE\n",
    "    )\n",
    "    \n",
    "    if X_train_enhanced is not None:\n",
    "        print(f\"Enhanced Training data shape: X_train: {X_train_enhanced.shape}, y_train: {y_train_enhanced.shape}\")\n",
    "        print(f\"Enhanced Testing data shape: X_test: {X_test_enhanced.shape}, y_test: {y_test_enhanced.shape}\")\n",
    "        print(f\"Enhanced features used: {feature_names_enhanced}\")\n",
    "        \n",
    "        # Build enhanced model\n",
    "        print(\"\\n🏗️ Building sentiment-enhanced LSTM model...\")\n",
    "        enhanced_model = build_lstm_model((X_train_enhanced.shape[1], X_train_enhanced.shape[2]))\n",
    "        print(\"Enhanced LSTM Model Architecture:\")\n",
    "        enhanced_model.summary()\n",
    "    else:\n",
    "        print(\"Failed to prepare enhanced data\")\n",
    "        enhanced_model = None\n",
    "else:\n",
    "    print(\"No enhanced stock data available\")\n",
    "    enhanced_model = None\n"
   ],
   "id": "e52ebfeb9f122942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train enhanced model\n",
    "if enhanced_model is not None and X_train_enhanced is not None:\n",
    "    print(\"\\n🚀 Training sentiment-enhanced LSTM model...\")\n",
    "    \n",
    "    # Train the enhanced model\n",
    "    enhanced_history = enhanced_model.fit(\n",
    "        X_train_enhanced, y_train_enhanced,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Enhanced model training completed!\")\n",
    "    \n",
    "    # Make enhanced predictions\n",
    "    print(\"\\n📊 Evaluating enhanced model...\")\n",
    "    enhanced_train_predictions = enhanced_model.predict(X_train_enhanced, verbose=0)\n",
    "    enhanced_test_predictions = enhanced_model.predict(X_test_enhanced, verbose=0)\n",
    "    \n",
    "    # Inverse transform predictions and actual values\n",
    "    enhanced_train_predictions_scaled = close_scaler.inverse_transform(enhanced_train_predictions)\n",
    "    enhanced_test_predictions_scaled = close_scaler.inverse_transform(enhanced_test_predictions.reshape(-1, 1))\n",
    "    y_train_enhanced_scaled = close_scaler.inverse_transform(y_train_enhanced.reshape(-1, 1))\n",
    "    y_test_enhanced_scaled = close_scaler.inverse_transform(y_test_enhanced.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate metrics for enhanced model\n",
    "    enhanced_lstm_metrics = calculate_metrics(y_test_enhanced_scaled, enhanced_test_predictions_scaled, 'Enhanced LSTM')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📈 REPORT 2: STOCK FORECAST USING TECHNICAL INDICATORS AND NEWS SENTIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create comprehensive comparison DataFrame\n",
    "    all_metrics_df = pd.DataFrame([baseline_lstm_metrics, enhanced_lstm_metrics, naive_baseline_metrics])\n",
    "    all_metrics_df = all_metrics_df.round(4)\n",
    "    \n",
    "    print(\"\\n📊 COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 60)\n",
    "    print(all_metrics_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvements\n",
    "    rmse_improvement_enhanced = ((baseline_lstm_metrics['RMSE'] - enhanced_lstm_metrics['RMSE']) / baseline_lstm_metrics['RMSE']) * 100\n",
    "    directional_improvement_enhanced = enhanced_lstm_metrics['Directional_Accuracy'] - baseline_lstm_metrics['Directional_Accuracy']\n",
    "    \n",
    "    print(f\"\\n📈 SENTIMENT ENHANCEMENT EFFECTIVENESS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"RMSE Improvement over Baseline LSTM: {rmse_improvement_enhanced:.2f}%\")\n",
    "    print(f\"Directional Accuracy Improvement: {directional_improvement_enhanced:.2f} percentage points\")\n",
    "    \n",
    "    # Overall comparison with naive baseline\n",
    "    rmse_improvement_vs_naive = ((naive_baseline_metrics['RMSE'] - enhanced_lstm_metrics['RMSE']) / naive_baseline_metrics['RMSE']) * 100\n",
    "    directional_improvement_vs_naive = enhanced_lstm_metrics['Directional_Accuracy'] - naive_baseline_metrics['Directional_Accuracy']\n",
    "    \n",
    "    print(f\"\\n📈 ENHANCED MODEL vs NAIVE BASELINE\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"RMSE Improvement over Naive Baseline: {rmse_improvement_vs_naive:.2f}%\")\n",
    "    print(f\"Directional Accuracy Improvement: {directional_improvement_vs_naive:.2f} percentage points\")\n",
    "    \n",
    "    # Determine effectiveness\n",
    "    if rmse_improvement_enhanced > 5:\n",
    "        print(\"\\n✅ SIGNIFICANT IMPROVEMENT: News sentiment significantly enhances prediction accuracy\")\n",
    "    elif rmse_improvement_enhanced > 0:\n",
    "        print(\"\\n✅ MODEST IMPROVEMENT: News sentiment provides modest enhancement\")\n",
    "    elif rmse_improvement_enhanced > -2:\n",
    "        print(\"\\n⚠️  MINIMAL IMPACT: News sentiment has minimal impact on prediction accuracy\")\n",
    "    else:\n",
    "        print(\"\\n❌ NEGATIVE IMPACT: News sentiment appears to hurt prediction accuracy\")\n",
    "\n",
    "else:\n",
    "    print(\"Enhanced model training skipped due to data preparation issues\")\n",
    "    enhanced_lstm_metrics = None\n",
    "    enhanced_test_predictions_scaled = None\n",
    "    y_test_enhanced_scaled = None\n"
   ],
   "id": "44bfa119db6fa299",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comprehensive visualization comparing both models\n",
    "if enhanced_model is not None and enhanced_test_predictions_scaled is not None:\n",
    "    print(\"\\n📊 Creating comprehensive model comparison visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
    "    fig.suptitle(f'{TARGET_STOCK} LSTM Models Comparison: Baseline vs Sentiment-Enhanced', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. Training history comparison\n",
    "    axes[0, 0].plot(baseline_history.history['loss'], label='Baseline Training Loss', color='blue', alpha=0.7)\n",
    "    axes[0, 0].plot(baseline_history.history['val_loss'], label='Baseline Validation Loss', color='blue', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].plot(enhanced_history.history['loss'], label='Enhanced Training Loss', color='red', alpha=0.7)\n",
    "    axes[0, 0].plot(enhanced_history.history['val_loss'], label='Enhanced Validation Loss', color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_title('Training History Comparison')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Predictions comparison\n",
    "    test_dates = enhanced_data.index[-len(y_test_baseline_scaled):]\n",
    "    axes[0, 1].plot(test_dates, y_test_baseline_scaled, label='Actual Price', color='black', linewidth=2)\n",
    "    axes[0, 1].plot(test_dates, baseline_test_predictions_scaled, label='Baseline LSTM', color='blue', linewidth=2, alpha=0.8)\n",
    "    axes[0, 1].plot(test_dates, enhanced_test_predictions_scaled, label='Enhanced LSTM', color='red', linewidth=2, alpha=0.8)\n",
    "    axes[0, 1].plot(test_dates, naive_predictions, label='Naive Baseline', color='green', linewidth=1, linestyle='--')\n",
    "    axes[0, 1].set_title('Stock Price Predictions Comparison')\n",
    "    axes[0, 1].set_ylabel('Price ($)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Error comparison\n",
    "    baseline_errors = y_test_baseline_scaled.flatten() - baseline_test_predictions_scaled.flatten()\n",
    "    enhanced_errors = y_test_enhanced_scaled.flatten() - enhanced_test_predictions_scaled.flatten()\n",
    "    \n",
    "    axes[0, 2].hist(baseline_errors, bins=30, alpha=0.7, label='Baseline LSTM Errors', color='blue')\n",
    "    axes[0, 2].hist(enhanced_errors, bins=30, alpha=0.7, label='Enhanced LSTM Errors', color='red')\n",
    "    axes[0, 2].set_title('Prediction Error Distribution')\n",
    "    axes[0, 2].set_xlabel('Prediction Error ($)')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Performance metrics comparison\n",
    "    metrics_comparison = pd.DataFrame([baseline_lstm_metrics, enhanced_lstm_metrics])\n",
    "    metrics_for_plot = metrics_comparison[['RMSE', 'MAE', 'Directional_Accuracy']]\n",
    "    \n",
    "    x_pos = np.arange(len(metrics_for_plot.columns))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x_pos - width/2, metrics_for_plot.iloc[0], width, label='Baseline LSTM', color='blue', alpha=0.7)\n",
    "    axes[1, 0].bar(x_pos + width/2, metrics_for_plot.iloc[1], width, label='Enhanced LSTM', color='red', alpha=0.7)\n",
    "    axes[1, 0].set_title('Performance Metrics Comparison')\n",
    "    axes[1, 0].set_xlabel('Metrics')\n",
    "    axes[1, 0].set_ylabel('Values')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(metrics_for_plot.columns)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (baseline_val, enhanced_val) in enumerate(zip(metrics_for_plot.iloc[0], metrics_for_plot.iloc[1])):\n",
    "        axes[1, 0].text(i - width/2, baseline_val + baseline_val*0.01, f'{baseline_val:.2f}', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "        axes[1, 0].text(i + width/2, enhanced_val + enhanced_val*0.01, f'{enhanced_val:.2f}', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 5. Scatter plot comparison\n",
    "    axes[1, 1].scatter(y_test_baseline_scaled, baseline_test_predictions_scaled, alpha=0.6, color='blue', label='Baseline LSTM')\n",
    "    axes[1, 1].scatter(y_test_enhanced_scaled, enhanced_test_predictions_scaled, alpha=0.6, color='red', label='Enhanced LSTM')\n",
    "    min_price = min(y_test_baseline_scaled.min(), enhanced_test_predictions_scaled.min())\n",
    "    max_price = max(y_test_baseline_scaled.max(), enhanced_test_predictions_scaled.max())\n",
    "    axes[1, 1].plot([min_price, max_price], [min_price, max_price], 'k--', alpha=0.8, label='Perfect Prediction')\n",
    "    axes[1, 1].set_title('Actual vs Predicted Prices')\n",
    "    axes[1, 1].set_xlabel('Actual Price ($)')\n",
    "    axes[1, 1].set_ylabel('Predicted Price ($)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Improvement summary\n",
    "    axes[1, 2].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    📊 PERFORMANCE SUMMARY\n",
    "    \n",
    "    Target Stock: {TARGET_STOCK}\n",
    "    Analysis Period: {START_DATE} to {END_DATE}\n",
    "    \n",
    "    🔹 Baseline LSTM:\n",
    "    RMSE: ${baseline_lstm_metrics['RMSE']:.2f}\n",
    "    Directional Accuracy: {baseline_lstm_metrics['Directional_Accuracy']:.1f}%\n",
    "    \n",
    "    🔹 Enhanced LSTM:\n",
    "    RMSE: ${enhanced_lstm_metrics['RMSE']:.2f}\n",
    "    Directional Accuracy: {enhanced_lstm_metrics['Directional_Accuracy']:.1f}%\n",
    "    \n",
    "    🔹 Improvement:\n",
    "    RMSE: {rmse_improvement_enhanced:.2f}%\n",
    "    Directional Accuracy: {directional_improvement_enhanced:.2f}pp\n",
    "    \n",
    "    🔹 vs Naive Baseline:\n",
    "    RMSE Improvement: {rmse_improvement_vs_naive:.2f}%\n",
    "    Directional Improvement: {directional_improvement_vs_naive:.2f}pp\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes, fontsize=11,\n",
    "                    verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✅ Report 2 completed for {TARGET_STOCK}\")\n",
    "    print(f\"Enhanced LSTM RMSE: ${enhanced_lstm_metrics['RMSE']:.2f}\")\n",
    "    print(f\"Enhanced LSTM Directional Accuracy: {enhanced_lstm_metrics['Directional_Accuracy']:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Comprehensive visualization skipped due to enhanced model issues\")"
   ],
   "id": "80a52987bebacfb9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
