{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stock Price Prediction with Sentiment Analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !pip install charset-normalizer pandas_ta yfinance statsmodels tqdm scikit-learn tensorflow\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To reload the imported modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Import modules\n",
    "import config as cfg\n",
    "import data_processing as dp\n",
    "import sentiment_analysis as sa\n",
    "import model as mdl\n",
    "import utils as ut"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(cfg.DATASET_DIR, exist_ok=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Load and Analyze News Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentiment_csv_path = f\"{cfg.DATASET_DIR}/{cfg.STOCK_SYMBOL}_daily_sentiment.csv\"\n",
    "\n",
    "if cfg.UPDATE_SENTIMENT_CSV or not os.path.exists(sentiment_csv_path):\n",
    "    print(\"Generating new sentiment data and saving to CSV...\")\n",
    "    news_df = dp.load_and_analyze_news_data(cfg.NEWS_DATA_FILE, cfg.STOCK_SYMBOL)\n",
    "    company_sentiment_df = sa.process_news_sentiment(news_df, cfg.STOCK_SYMBOL)\n",
    "    daily_sentiment_df = sa.aggregate_daily_sentiment(company_sentiment_df)\n",
    "    ut.save_dataframe(daily_sentiment_df, sentiment_csv_path)\n",
    "else:\n",
    "    print(f\"Loading existing sentiment data from {sentiment_csv_path}...\")\n",
    "    daily_sentiment_df = pd.read_csv(sentiment_csv_path, index_col='Date', parse_dates=True)\n",
    "\n",
    "print(\"Sentiment data ready.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Fetch Stock Data and Calculate Technical Indicators"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "START_DATE, END_DATE = ut.calculate_dynamic_date_range(daily_sentiment_df)\n",
    "\n",
    "stock_filename = f\"{cfg.STOCK_SYMBOL}_stock_data_{START_DATE}_to_{END_DATE}.csv\"\n",
    "stock_csv_path = os.path.join(cfg.DATASET_DIR, stock_filename)\n",
    "\n",
    "if cfg.UPDATE_STOCK_CSV or not os.path.exists(stock_csv_path):\n",
    "    print(f\"Fetching new stock data from yfinance ({START_DATE} to {END_DATE})...\")\n",
    "    stock_data = dp.fetch_stock_data(cfg.STOCK_SYMBOL, START_DATE, END_DATE)\n",
    "    if stock_data is not None:\n",
    "        stock_data.to_csv(stock_csv_path)\n",
    "        print(f\"Stock data saved to {stock_csv_path}\")\n",
    "else:\n",
    "    print(f\"Loading existing stock data from {stock_csv_path}...\")\n",
    "    stock_data = pd.read_csv(stock_csv_path, index_col='Date', parse_dates=True)\n",
    "\n",
    "# Calculate technical indicators from the loaded/fetched data\n",
    "tech_data = dp.calculate_technical_indicators(stock_data)\n",
    "print(\"Technical indicators calculated.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Prepare Data for LSTM & SVM Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare data for models using only technical indicators\n",
    "X_train_tech, X_test_tech, y_train_tech, y_test_tech, scaler_tech = mdl.prepare_data_for_lstm(\n",
    "    tech_data, cfg.BASELINE_FEATURES, cfg.BASELINE_TARGET, cfg.SEQUENCE_LENGTH, cfg.TEST_SIZE\n",
    ")\n",
    "\n",
    "# Prepare data for models using sentiment + technical indicators\n",
    "enhanced_full_data = dp.create_enhanced_dataset(tech_data, daily_sentiment_df)\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh, scaler_enh = mdl.prepare_data_for_lstm(\n",
    "    enhanced_full_data, cfg.ENHANCED_FEATURES, cfg.ENHANCED_TARGET, cfg.SEQUENCE_LENGTH, cfg.TEST_SIZE\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Run LSTM Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Running Baseline LSTM Model (Single-Layer) ---\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=0)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "baseline_model = mdl.build_single_layer_lstm((X_train_tech.shape[1], X_train_tech.shape[2]))\n",
    "baseline_history = baseline_model.fit(\n",
    "    X_train_tech, y_train_tech, epochs=cfg.EPOCHS, batch_size=cfg.BATCH_SIZE,\n",
    "    validation_split=0.2, callbacks=[reduce_lr, early_stopping], verbose=0\n",
    ")\n",
    "print(\"Baseline LSTM training complete.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_preds = baseline_model.predict(X_test_tech, verbose=0)\n",
    "close_scaler = MinMaxScaler().fit(tech_data[['Close']]) # Use a simple scaler for just the target column\n",
    "y_test_tech_scaled = close_scaler.inverse_transform(y_test_tech.reshape(-1, 1))\n",
    "base_preds_scaled = close_scaler.inverse_transform(base_preds)\n",
    "baseline_metrics = ut.calculate_metrics(y_test_tech_scaled, base_preds_scaled, 'Baseline LSTM')\n",
    "test_dates_tech = tech_data.index[-len(y_test_tech_scaled):]\n",
    "ut.plot_model_results(baseline_history, y_test_tech_scaled, base_preds_scaled, test_dates_tech, cfg.STOCK_SYMBOL, 'Baseline LSTM')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Running Multi-Layer LSTM Model ---\")\n",
    "multi_layer_model = mdl.build_multi_layer_lstm((X_train_tech.shape[1], X_train_tech.shape[2]))\n",
    "multi_layer_history = multi_layer_model.fit(\n",
    "    X_train_tech, y_train_tech, epochs=cfg.EPOCHS, batch_size=cfg.BATCH_SIZE,\n",
    "    validation_split=0.2, callbacks=[reduce_lr, early_stopping], verbose=0\n",
    ")\n",
    "print(\"Multi-Layer LSTM training complete.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multi_preds = multi_layer_model.predict(X_test_tech, verbose=0)\n",
    "multi_preds_scaled = close_scaler.inverse_transform(multi_preds)\n",
    "multi_layer_metrics = ut.calculate_metrics(y_test_tech_scaled, multi_preds_scaled, 'Multi-Layer LSTM')\n",
    "ut.plot_model_results(multi_layer_history, y_test_tech_scaled, multi_preds_scaled, test_dates_tech, cfg.STOCK_SYMBOL, 'Multi-Layer LSTM')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Running Enhanced LSTM Model (Single-Layer) ---\")\n",
    "enhanced_model = mdl.build_single_layer_lstm((X_train_enh.shape[1], X_train_enh.shape[2]))\n",
    "enhanced_history = enhanced_model.fit(\n",
    "    X_train_enh, y_train_enh, epochs=cfg.EPOCHS, batch_size=cfg.BATCH_SIZE,\n",
    "    validation_split=0.2, callbacks=[reduce_lr, early_stopping], verbose=0\n",
    ")\n",
    "print(\"Enhanced LSTM training complete.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "enh_preds = enhanced_model.predict(X_test_enh, verbose=0)\n",
    "close_scaler_enh = MinMaxScaler().fit(enhanced_full_data[['Close']])\n",
    "y_test_enh_scaled = close_scaler_enh.inverse_transform(y_test_enh.reshape(-1, 1))\n",
    "enh_preds_scaled = close_scaler_enh.inverse_transform(enh_preds)\n",
    "enhanced_metrics = ut.calculate_metrics(y_test_enh_scaled, enh_preds_scaled, 'Enhanced LSTM')\n",
    "test_dates_enh = enhanced_full_data.index[-len(y_test_enh_scaled):]\n",
    "ut.plot_model_results(enhanced_history, y_test_enh_scaled, enh_preds_scaled, test_dates_enh, cfg.STOCK_SYMBOL, 'Enhanced LSTM')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Running Multi-Layer Enhanced LSTM Model ---\")\n",
    "multi_enhanced_model = mdl.build_multi_layer_lstm((X_train_enh.shape[1], X_train_enh.shape[2]))\n",
    "multi_enhanced_history = multi_enhanced_model.fit(\n",
    "    X_train_enh, y_train_enh, epochs=cfg.EPOCHS, batch_size=cfg.BATCH_SIZE,\n",
    "    validation_split=0.2, callbacks=[reduce_lr, early_stopping], verbose=0\n",
    ")\n",
    "print(\"Multi-Layer Enhanced LSTM training complete.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multi_enh_preds = multi_enhanced_model.predict(X_test_enh, verbose=0)\n",
    "multi_enh_preds_scaled = close_scaler_enh.inverse_transform(multi_enh_preds)\n",
    "multi_enhanced_metrics = ut.calculate_metrics(y_test_enh_scaled, multi_enh_preds_scaled, 'Multi-Layer Enhanced LSTM')\n",
    "ut.plot_model_results(multi_enhanced_history, y_test_enh_scaled, multi_enh_preds_scaled, test_dates_enh, cfg.STOCK_SYMBOL, 'Multi-Layer Enhanced LSTM')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Run Support Vector Machine (SVM) Models --- (NEW)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Preparing Data for SVM ---\")\n",
    "# SVM requires 2D input, so we flatten the sequence data\n",
    "nsamples, nx, ny = X_train_tech.shape\n",
    "X_train_svm_tech = X_train_tech.reshape((nsamples, nx * ny))\n",
    "\n",
    "nsamples, nx, ny = X_test_tech.shape\n",
    "X_test_svm_tech = X_test_tech.reshape((nsamples, nx * ny))\n",
    "\n",
    "print(f\"Reshaped baseline data for SVM: {X_train_svm_tech.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_svm_model = mdl.build_and_train_svm(X_train_svm_tech, y_train_tech)\n",
    "base_svm_preds = baseline_svm_model.predict(X_test_svm_tech)\n",
    "\n",
    "base_svm_preds_scaled = close_scaler.inverse_transform(base_svm_preds.reshape(-1, 1))\n",
    "baseline_svm_metrics = ut.calculate_metrics(y_test_tech_scaled, base_svm_preds_scaled, 'Baseline SVM')\n",
    "ut.plot_non_keras_results(y_test_tech_scaled, base_svm_preds_scaled, test_dates_tech, cfg.STOCK_SYMBOL, 'Baseline SVM')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Preparing Enhanced Data for SVM ---\")\n",
    "# Reshape the enhanced data\n",
    "nsamples, nx, ny = X_train_enh.shape\n",
    "X_train_svm_enh = X_train_enh.reshape((nsamples, nx * ny))\n",
    "\n",
    "nsamples, nx, ny = X_test_enh.shape\n",
    "X_test_svm_enh = X_test_enh.reshape((nsamples, nx * ny))\n",
    "\n",
    "print(f\"Reshaped enhanced data for SVM: {X_train_svm_enh.shape}\")\n",
    "\n",
    "enhanced_svm_model = mdl.build_and_train_svm(X_train_svm_enh, y_train_enh)\n",
    "enh_svm_preds = enhanced_svm_model.predict(X_test_svm_enh)\n",
    "\n",
    "enh_svm_preds_scaled = close_scaler_enh.inverse_transform(enh_svm_preds.reshape(-1, 1))\n",
    "enhanced_svm_metrics = ut.calculate_metrics(y_test_enh_scaled, enh_svm_preds_scaled, 'Enhanced SVM')\n",
    "ut.plot_non_keras_results(y_test_enh_scaled, enh_svm_preds_scaled, test_dates_enh, cfg.STOCK_SYMBOL, 'Enhanced SVM')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Run ARIMA Model --- (NEW)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Running ARIMA Model ---\")\n",
    "# ARIMA works on the unscaled, 1D time series of the target variable\n",
    "close_prices = tech_data[cfg.BASELINE_TARGET]\n",
    "train_size = int(len(close_prices) * (1 - cfg.TEST_SIZE))\n",
    "train_arima, test_arima = close_prices[0:train_size], close_prices[train_size:]\n",
    "\n",
    "history = [x for x in train_arima]\n",
    "arima_predictions = []\n",
    "\n",
    "print(f\"Performing rolling forecast for {len(test_arima)} steps...\")\n",
    "for t in range(len(test_arima)):\n",
    "    model_arima = mdl.build_and_train_arima(history)\n",
    "    output = model_arima.forecast()\n",
    "    yhat = output[0]\n",
    "    arima_predictions.append(yhat)\n",
    "    obs = test_arima[t]\n",
    "    history.append(obs)\n",
    "\n",
    "arima_predictions_np = np.array(arima_predictions).reshape(-1, 1)\n",
    "y_test_arima_np = np.array(test_arima).reshape(-1, 1)\n",
    "arima_metrics = ut.calculate_metrics(y_test_arima_np, arima_predictions_np, 'ARIMA')\n",
    "ut.plot_non_keras_results(y_test_arima_np, arima_predictions_np, test_arima.index, cfg.STOCK_SYMBOL, 'ARIMA')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Final Performance Comparison --- (UPDATED)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "naive_preds = np.roll(y_test_tech_scaled, 1)\n",
    "naive_preds[0] = y_test_tech_scaled[0]\n",
    "naive_metrics = ut.calculate_metrics(y_test_tech_scaled, naive_preds, 'Naive Baseline')\n",
    "\n",
    "# --- UPDATED TO INCLUDE ALL MODELS ---\n",
    "all_metrics_df = pd.DataFrame([\n",
    "    naive_metrics,\n",
    "    baseline_metrics,\n",
    "    enhanced_metrics,\n",
    "    multi_layer_metrics,\n",
    "    multi_enhanced_metrics,\n",
    "    baseline_svm_metrics,\n",
    "    enhanced_svm_metrics,\n",
    "    arima_metrics\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\nüìä COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 60)\n",
    "print(all_metrics_df.to_string(index=False))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "metrics_to_evaluate = {\n",
    "    'RMSE': 'min',\n",
    "    'MAE': 'min',\n",
    "    'Directional_Accuracy': 'max'\n",
    "}\n",
    "\n",
    "print(\"\\nüèÜ Best Model for Each Metric\")\n",
    "print(\"-\" * 30)\n",
    "for metric, method in metrics_to_evaluate.items():\n",
    "    if method == 'min':\n",
    "        winner_idx = all_metrics_df[metric].idxmin()\n",
    "    else:\n",
    "        winner_idx = all_metrics_df[metric].idxmax()\n",
    "    winner_row = all_metrics_df.loc[winner_idx]\n",
    "    print(f\"{metric:<22}: {winner_row['Model']} (Score: {winner_row[metric]:.4f})\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- UPDATED TO INCLUDE ALL MODELS ---\n",
    "plot_data = {\n",
    "    'Actual': {'dates': test_dates_tech, 'values': y_test_tech_scaled},\n",
    "    'Baseline LSTM': {'dates': test_dates_tech, 'values': base_preds_scaled},\n",
    "    'Multi-Layer LSTM': {'dates': test_dates_tech, 'values': multi_preds_scaled},\n",
    "    'Enhanced LSTM': {'dates': test_dates_enh, 'values': enh_preds_scaled},\n",
    "    'Multi-Layer Enhanced LSTM': {'dates': test_dates_enh, 'values': multi_enh_preds_scaled},\n",
    "    'Baseline SVM': {'dates': test_dates_tech, 'values': base_svm_preds_scaled},\n",
    "    'Enhanced SVM': {'dates': test_dates_enh, 'values': enh_svm_preds_scaled},\n",
    "    'ARIMA': {'dates': test_arima.index, 'values': arima_predictions_np}\n",
    "}\n",
    "ut.plot_final_comparison(plot_data, cfg.STOCK_SYMBOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}